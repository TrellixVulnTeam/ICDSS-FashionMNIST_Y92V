{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kFz3FFLu4UB"
   },
   "source": [
    "# Transfer Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE4pA-wyYOGm"
   },
   "source": [
    "## Google Drive Settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "weFYX_tO4Tzf",
    "outputId": "c1bf60f7-506e-4f7a-e969-11f814fba379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/icdss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OQG_cI91YNc2",
    "outputId": "8299b4c9-f3aa-4b08-e9ee-403d9940fe99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zoT9s8TYZG7"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SM3W1c_D4r7k",
    "outputId": "c9f46aca-9f1c-45a9-d368-3a8100e5e7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from reader import get_images\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "iyeWkiEi4vIh",
    "outputId": "da99609d-e597-401a-988e-e5d38f50836b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/icdss/reader.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  x_train = df_train.drop('label', axis=1).as_matrix().astype(np.uint8)\n",
      "/content/drive/My Drive/Colab Notebooks/icdss/reader.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_train = df_train['label'].as_matrix().astype(np.uint8)\n",
      "/content/drive/My Drive/Colab Notebooks/icdss/reader.py:40: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  x_test = df_test.drop('label', axis=1).as_matrix().astype(np.uint8)\n",
      "/content/drive/My Drive/Colab Notebooks/icdss/reader.py:41: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  y_test = df_test['label'].as_matrix().astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "(x_train, y_train_raw), (x_test, y_test_raw) = get_images() \n",
    "\n",
    "#convert training and test images into three channels\n",
    "x_train = np.dstack([x_train]*3)\n",
    "x_test = np.dstack([x_test]*3)\n",
    "\n",
    "# Reshape data to image's dimension 28 by 28\n",
    "x_train = x_train.reshape(-1,28,28,3)\n",
    "x_test = x_test.reshape(-1,28,28,3)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5iR9KPB045w5",
    "outputId": "5b50c410-5365-4ce1-81a1-ca8f19eb7b1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 28, 28, 3), (12000, 28, 28, 3), (48000, 10), (12000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change classes to a categorical type\n",
    "num_classes = 10\n",
    "y_train = utils.to_categorical(y_train_raw, num_classes)\n",
    "y_test = utils.to_categorical(y_test_raw, num_classes)\n",
    "\n",
    "# split original training data to 80 % training data and the other 20% for validation data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=13)\n",
    "\n",
    "# Check the data size\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "830jQsMha0Mp"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ms9t_GNa3Jc"
   },
   "outputs": [],
   "source": [
    "# Random Erasing Data Augmentation\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.1, r_1=0.1, r_2=1/0.1, v_l=0, v_h=1, pixel_level=True):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "V1pZvFN-vOTr",
    "outputId": "3d9f119c-4896-4852-9635-35f84d30f032"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAHBCAYAAADpW/sfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAhEBZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eraser = get_random_eraser()\n",
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "\n",
    "for i, img in enumerate(x_train[:16]):\n",
    "    # Set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(4, 4, i + 1)\n",
    "    sp.axis('Off') # Don't show axes (or gridlines)\n",
    "    \n",
    "    plt.imshow(eraser(img/255))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jKP1ImZx497v",
    "outputId": "af6f355c-23f9-48ba-c635-dcd8838a4c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing generator for train dataset\n",
      "Preparing generator for validation dataset\n"
     ]
    }
   ],
   "source": [
    "# All images will be rescaled by 1./255. We apply data augmentation here.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   horizontal_flip=True,\n",
    "                                   width_shift_range=0.1,  \n",
    "                                   height_shift_range=0.1, \n",
    "                                   fill_mode='nearest',\n",
    "                                   preprocessing_function=get_random_eraser(v_l=0, v_h=1))\n",
    "val_datagen = ImageDataGenerator(rescale=1./255,  \n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "print(\"Preparing generator for train dataset\")\n",
    "train_generator = train_datagen.flow(x_train, \n",
    "                                     y_train, \n",
    "                                     batch_size=bs)\n",
    "\n",
    "# Flow validation images in batches of 32 using val_datagen generator\n",
    "print(\"Preparing generator for validation dataset\")\n",
    "val_generator = val_datagen.flow(x_val, \n",
    "                                 y_val,\n",
    "                                 batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2rhIBoGbeZy"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ou1Uga7n5aFL",
    "outputId": "2bf6dd70-3cd3-47dc-b00e-014bf17192f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 3s 0us/step\n",
      "Number of layers in the base model:  311\n",
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 150, 150, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 74, 74, 32)   864         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 74, 74, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 74, 74, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 72, 72, 32)   9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 72, 72, 32)   96          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 72, 72, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 72, 72, 64)   18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 72, 72, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 72, 72, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 35, 35, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 80)   240         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 35, 35, 80)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 33, 33, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 33, 33, 192)  576         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 33, 33, 192)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 48)   144         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 96)   288         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 48)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 96)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 64)   192         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 96)   288         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   96          conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 96)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 48)   144         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 48)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 64)   192         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 64)   192         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 64)   192         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 48)   144         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 96)   288         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 48)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 64)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 64)   192         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 96)   288         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 64)   192         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 64)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 64)   192         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 64)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 96)   288         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 96)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 7, 7, 96)     82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 384)    1152        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 96)     288         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 7, 7, 384)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 7, 7, 96)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 128)    384         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 128)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 128)    114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 128)    384         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 128)    384         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 7, 7, 128)    384         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 128)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 128)    114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 128)    384         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 128)    384         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 192)    172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 192)    172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 192)    576         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 192)    576         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 192)    576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 192)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 192)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 192)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 160)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 7, 7, 160)    480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 7, 7, 160)    480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 160)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 160)    179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 7, 7, 160)    480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 7, 7, 160)    480         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 7, 7, 192)    215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 7, 7, 192)    215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 192)    576         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 7, 7, 192)    576         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 192)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 192)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 7, 7, 160)    480         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 7, 7, 160)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 7, 7, 160)    179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 160)    480         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 160)    480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 7, 7, 160)    480         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 7, 7, 160)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 160)    179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 160)    480         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 7, 7, 160)    480         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 7, 7, 192)    215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 7, 7, 192)    215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 7, 7, 192)    576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 7, 7, 192)    576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 7, 7, 192)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 192)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 7, 7, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 7, 7, 192)    258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 7, 7, 192)    576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 7, 7, 192)    576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 7, 7, 192)    576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 7, 7, 192)    258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 7, 7, 192)    576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 7, 7, 192)    576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 7, 7, 192)    576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 7, 7, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 7, 7, 192)    576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 7, 7, 192)    576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 7, 7, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 7, 7, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 7, 7, 192)    258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 7, 7, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 7, 7, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 7, 7, 192)    258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 7, 7, 192)    576         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 7, 7, 192)    576         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 7, 7, 192)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 7, 7, 192)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 3, 3, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 3, 3, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 3, 3, 320)    960         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 3, 3, 192)    576         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 3, 3, 320)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 3, 3, 192)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 3, 3, 448)    1344        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 3, 3, 448)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 3, 3, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 3, 3, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 3, 3, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 3, 3, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 3, 3, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 3, 3, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 3, 3, 384)    1152        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 3, 3, 384)    1152        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 3, 3, 320)    960         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 3, 3, 192)    576         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 3, 3, 320)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 3, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 192)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 3, 3, 448)    1344        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 3, 3, 448)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 3, 3, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 3, 3, 384)    1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 3, 3, 384)    1152        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 384)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 3, 3, 384)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 3, 3, 384)    1152        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 3, 3, 384)    1152        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 3, 3, 384)    1152        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 3, 3, 384)    1152        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 3, 3, 320)    960         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 3, 3, 192)    576         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 320)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 3, 3, 192)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here we specify the input shape of our data \n",
    "# This should match the size of images ('image_size') along with the number of channels (3)\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 10\n",
    "\n",
    "#ResNet50 Transfer Learning\n",
    "base_model = tf.keras.applications.InceptionV3(include_top=False, \n",
    "                                            weights='imagenet', \n",
    "                                            input_shape=input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "EFglWrB55y-Q",
    "outputId": "966495cc-f5cc-4581-91c3-10b23e8b1626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 28, 28, 3)]       0         \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 22,857,002\n",
      "Trainable params: 22,822,570\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = tf.keras.Input(shape=(28, 28, 3))\n",
    "\n",
    "x = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (150, 150)))(input_img)\n",
    "\n",
    "x = base_model(x)\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(512)(x) \n",
    "\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_img, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3T3MuMmv084j"
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHNxRqpPGDtR"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.5e-6)\n",
    "bestValidationCheckpointer = ModelCheckpoint('train_model_inception_net.hdf5', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zekebnp7jQSk"
   },
   "outputs": [],
   "source": [
    "Adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
    "SGD = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "fkt2dTP02PLe",
    "outputId": "c722636d-da0d-47ce-884d-9db80e39d6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1499/1500 [============================>.] - ETA: 0s - loss: 1.3795 - accuracy: 0.5447\n",
      "Epoch 00001: val_loss improved from inf to 1.48233, saving model to train_model_check.hdf5\n",
      "1500/1500 [==============================] - 429s 286ms/step - loss: 1.3794 - accuracy: 0.5447 - val_loss: 1.4823 - val_accuracy: 0.6014\n",
      "Epoch 2/3\n",
      "  47/1500 [..............................] - ETA: 6:26 - loss: 1.2348 - accuracy: 0.5718"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8b56c564f513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m12000\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestValidationCheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[1;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_zeros\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    639\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m     \u001b[0mshape_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mis_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    946\u001b[0m   \"\"\"\n\u001b[1;32m    947\u001b[0m   return (isinstance(x, tensor_like._TensorLike) or  # pylint: disable=protected-access\n\u001b[0;32m--> 948\u001b[0;31m           \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m           getattr(x, \"is_tensor_like\", False))\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                              epochs=3,\n",
    "                              validation_data=val_generator, \n",
    "                              callbacks=[bestValidationCheckpointer, reduce_lr],\n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EgHtFbs1mcW"
   },
   "outputs": [],
   "source": [
    "# Plot the chart for accuracy and loss on both training and validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMLF46Tm1WAc"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uU6mH9bvjs4I"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:50]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fmJxc6LR1rKq",
    "outputId": "f0a0dbef-ae9e-4419-8037-b16ced5f99d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7364\n",
      "Epoch 00001: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 775s 1s/step - loss: 0.7372 - acc: 0.7365 - val_loss: 1.5057 - val_acc: 0.5104\n",
      "Epoch 2/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.6601 - acc: 0.7615\n",
      "Epoch 00002: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 773s 1s/step - loss: 0.6601 - acc: 0.7615 - val_loss: 1.4615 - val_acc: 0.5304\n",
      "Epoch 3/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.6231 - acc: 0.7711\n",
      "Epoch 00003: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 774s 1s/step - loss: 0.6229 - acc: 0.7711 - val_loss: 1.3805 - val_acc: 0.5516\n",
      "Epoch 4/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.7842\n",
      "Epoch 00004: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 773s 1s/step - loss: 0.5925 - acc: 0.7842 - val_loss: 1.3136 - val_acc: 0.5709\n",
      "Epoch 5/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.7924\n",
      "Epoch 00005: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 773s 1s/step - loss: 0.5636 - acc: 0.7925 - val_loss: 1.2885 - val_acc: 0.5848\n",
      "Epoch 6/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7989\n",
      "Epoch 00006: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 771s 1s/step - loss: 0.5449 - acc: 0.7989 - val_loss: 1.2781 - val_acc: 0.5918\n",
      "Epoch 7/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8021\n",
      "Epoch 00007: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 771s 1s/step - loss: 0.5342 - acc: 0.8022 - val_loss: 1.2193 - val_acc: 0.6022\n",
      "Epoch 8/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8089\n",
      "Epoch 00008: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 771s 1s/step - loss: 0.5223 - acc: 0.8089 - val_loss: 1.1995 - val_acc: 0.6123\n",
      "Epoch 9/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.8121\n",
      "Epoch 00009: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 769s 1s/step - loss: 0.5135 - acc: 0.8119 - val_loss: 1.1670 - val_acc: 0.6169\n",
      "Epoch 10/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8150\n",
      "Epoch 00010: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 769s 1s/step - loss: 0.5076 - acc: 0.8151 - val_loss: 1.1738 - val_acc: 0.6210\n",
      "Epoch 11/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8220\n",
      "Epoch 00011: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 769s 1s/step - loss: 0.4873 - acc: 0.8221 - val_loss: 1.1802 - val_acc: 0.6238\n",
      "Epoch 12/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8197\n",
      "Epoch 00012: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 771s 1s/step - loss: 0.4860 - acc: 0.8197 - val_loss: 1.1775 - val_acc: 0.6270\n",
      "Epoch 13/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8262\n",
      "Epoch 00013: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 770s 1s/step - loss: 0.4728 - acc: 0.8263 - val_loss: 1.1543 - val_acc: 0.6313\n",
      "Epoch 14/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8258\n",
      "Epoch 00014: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 771s 1s/step - loss: 0.4727 - acc: 0.8258 - val_loss: 1.1449 - val_acc: 0.6322\n",
      "Epoch 15/20\n",
      "749/750 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8315\n",
      "Epoch 00015: val_loss did not improve from 0.93525\n",
      "750/750 [==============================] - 770s 1s/step - loss: 0.4607 - acc: 0.8315 - val_loss: 1.1460 - val_acc: 0.6332\n",
      "Epoch 16/20\n",
      "423/750 [===============>..............] - ETA: 5:17 - loss: 0.4571 - acc: 0.8303"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD,\n",
    "              metrics=['acc'])\n",
    "\n",
    "history2 = model.fit_generator(train_generator, \n",
    "                               epochs=3,\n",
    "                               validation_data=val_generator, \n",
    "                               callbacks=[bestValidationCheckpointer, reduce_lr],\n",
    "                               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gk2FTkFb7zh7"
   },
   "outputs": [],
   "source": [
    "# Plot the chart for accuracy and loss on both training and validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsstaq7lHyM4"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_inception_icdss",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
